{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Embedding \"Word2Vec\"\n",
    "==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#System path 등록\n",
    "import sys  \n",
    "sys.path.insert(0, '../../tensorflow_keras_example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimhyungrak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kimhyungrak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kimhyungrak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kimhyungrak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kimhyungrak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kimhyungrak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: http://mattmahoney.net/dc/text8.zip\n",
      "Downloaded : ./datasets/text8/text8.zip ( 31344016 bytes)\n",
      "Train: [5233 3083   11    5  194]\n",
      "Vocabulary Length =  253854\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from datasetslib.text8 import Text8\n",
    "text8 = Text8()\n",
    "# downloads data, converts words to ids, converts files to a list of ids\n",
    "text8.load_data()\n",
    "print('Train:', text8.part['train'][0:5])\n",
    "# print(text8.part['test'][0:5])\n",
    "# print(text8.part['valid'][0:5])\n",
    "print('Vocabulary Length = ', text8.vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The skip-gram pairs : target,context\n",
      "( 11 as , 5233 anarchism )\n",
      "( 11 as , 3083 originated )\n",
      "( 11 as , 5 a )\n",
      "( 11 as , 194 term )\n",
      "( 5 a , 3083 originated )\n",
      "( 5 a , 11 as )\n",
      "( 5 a , 194 term )\n",
      "( 5 a , 1 of )\n",
      "( 194 term , 11 as )\n",
      "( 194 term , 5 a )\n"
     ]
    }
   ],
   "source": [
    "#skip_gram example\n",
    "text8.skip_window = 2\n",
    "text8.reset_index()\n",
    "# in skip-gram input is the target word and output is the context word\n",
    "x_batch, y_batch = text8.next_batch_sg()\n",
    "\n",
    "print('The skip-gram pairs : target,context')\n",
    "for i in range(5 * text8.skip_window):\n",
    "    print('(', x_batch[i], text8.id2word[x_batch[i]],\n",
    "          ',', y_batch[i], text8.id2word[y_batch[i]], ')')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7 32 64 53 41  3 43 52]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "valid_size = 8\n",
    "x_valid = np.random.choice(valid_size * 10, valid_size, replace=False)\n",
    "print(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec Network value Define\n",
    "\"\"\"\n",
    "Negative smapling : \n",
    "Word Embedding 된 Metrics 는 [Voca_size, Embedding _size] 인데 \n",
    "이를 Word2vec Skip-Gram 학습을 위해서 Voca_size 만큼 Softmax를 해야한다\n",
    "당연히 연산량이 크므로 이를 해결하기 위해서 Negative Sampling 사용 \n",
    "쉽게 말해서 모두 연산하지말고 Skip_window size(원래 맞추고자 하는 정답크기) + (정답과 거리가 먼 단어 5~20개를 샘플링하여)\n",
    "Softmax함 이 예제에서는 64개, Negative 샘플링 사이즈는 조절가능\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "embedding_hidden = 128\n",
    "batch_size = 128\n",
    "n_negative_sample= 64\n",
    "text8.skip_window=2\n",
    "n_epochs = 100\n",
    "learning_rate = 0.9\n",
    "text8.reset_index()\n",
    "n_batch = text8.n_batches_wv()\n",
    "\n",
    "input = tf.compat.v1.placeholder(dtype=tf.int32, shape=[batch_size])\n",
    "output = tf.compat.v1.placeholder(dtype=tf.int32, shape=[batch_size, 1])\n",
    "validation = tf.compat.v1.constant(x_valid,dtype=tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/kimhyungrak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#embedding network setup\n",
    "random_uniform_dist = tf.random.uniform(shape=[text8.vocab_len, embedding_hidden], \n",
    "                                       minval=-1.0, maxval=1.0)\n",
    "embedding_dist = tf.compat.v1.Variable(random_uniform_dist, name='embedding_matrix')\n",
    "embedded_table = tf.nn.embedding_lookup(embedding_dist, ids=input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise contrastive estimate loss \n",
    "nce_w = tf.compat.v1.Variable(tf.random.truncated_normal(shape=[text8.vocab_len, embedding_hidden], \n",
    "                                                         stddev=1/tf.sqrt(embedding_hidden*1.0)),\n",
    "                              name='nce_weight')\n",
    "\n",
    "nce_b = tf.compat.v1.Variable(tf.zeros(shape=[text8.vocab_len]), name='nce_biases')\n",
    "\n",
    "loss = tf.compat.v1.reduce_mean(tf.nn.nce_loss(weights=nce_w, biases=nce_b, inputs=embedded_table, labels=output, \n",
    "                     num_sampled=n_negative_sample, num_classes=text8.vocab_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/kimhyungrak/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "#Similarity Calculation\n",
    "normal_embedding_table = embedding_dist/tf.sqrt(tf.reduce_sum(tf.square(embedding_dist), axis=1, keepdims=True))\n",
    "validate_embedding_table = tf.nn.embedding_lookup(normal_embedding_table, validation)\n",
    "similarity = tf.matmul(validate_embedding_table, normal_embedding_table, transpose_b=True)\n",
    "\n",
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul_1:0' shape=(8, 253854) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 ,,,,avg_loss : 135.13171805351956\n"
     ]
    }
   ],
   "source": [
    "from datasetslib import nputil\n",
    "text8.reset_index()\n",
    "\n",
    "with tf.compat.v1.Session() as sess: \n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    for i in range(n_epochs):\n",
    "        avg_loss = 0\n",
    "        for batch in range(n_batch):\n",
    "            x_train, label = text8.next_batch_sg()\n",
    "            label = nputil.to2d(label, unit_axis=1)\n",
    "            res_loss, _ = sess.run([loss, optimizer], feed_dict={input:x_train, output:label})\n",
    "            avg_loss+=res_loss\n",
    "        print(f'epoch : {i} ,,,,avg_loss : {avg_loss/n_batch}')\n",
    "    \n",
    "    similarity_score = sess.run(similarity)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#similarity score calculation Test\n",
    "top_k = 5\n",
    "\n",
    "for i in range(valid_size):\n",
    "    similarity_cosine = similarity_score[i]\n",
    "    top_rank_5 = np.argsort(similarity_cosine)\n",
    "    start_num = text8.vocab_len - top_k-1\n",
    "    end_num = text8.vocab_len-1\n",
    "    top_rank_5_index = top_rank_5.tolist()[start_num:end_num]\n",
    "\n",
    "    similar_str = 'Similar to {0:}:'.format(text8.id2word[x_valid[i]])\n",
    "    for rank_word in top_rank_5_index:\n",
    "        similar_str = '{0:} {1:},'.format(similar_str, text8.id2word[rank_word])\n",
    "    \n",
    "    print(similar_str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}