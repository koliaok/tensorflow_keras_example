{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Embedding \"Word2Vec\"\n",
    "==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#System path 등록\n",
    "import sys  \n",
    "sys.path.insert(0, '../../../tensorflow_keras_example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists: ./datasets\\text8\\text8.zip\n",
      "Train: [5233 3083   11    5  194]\n",
      "Vocabulary Length =  253854\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from datasetslib.text8 import Text8\n",
    "text8 = Text8()\n",
    "# downloads data, converts words to ids, converts files to a list of ids\n",
    "text8.load_data()\n",
    "print('Train:', text8.part['train'][0:5])\n",
    "# print(text8.part['test'][0:5])\n",
    "# print(text8.part['valid'][0:5])\n",
    "print('Vocabulary Length = ', text8.vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The skip-gram pairs : target,context\n",
      "( 11 as , 5233 anarchism )\n",
      "( 11 as , 3083 originated )\n",
      "( 11 as , 5 a )\n",
      "( 11 as , 194 term )\n",
      "( 5 a , 3083 originated )\n",
      "( 5 a , 11 as )\n",
      "( 5 a , 194 term )\n",
      "( 5 a , 1 of )\n",
      "( 194 term , 11 as )\n",
      "( 194 term , 5 a )\n"
     ]
    }
   ],
   "source": [
    "#skip_gram example\n",
    "text8.skip_window = 2\n",
    "text8.reset_index()\n",
    "# in skip-gram input is the target word and output is the context word\n",
    "x_batch, y_batch = text8.next_batch_sg()\n",
    "\n",
    "print('The skip-gram pairs : target,context')\n",
    "for i in range(5 * text8.skip_window):\n",
    "    print('(', x_batch[i], text8.id2word[x_batch[i]],\n",
    "          ',', y_batch[i], text8.id2word[y_batch[i]], ')')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70 43 63 50 27 75 25 29]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "valid_size = 8\n",
    "x_valid = np.random.choice(valid_size * 10, valid_size, replace=False)\n",
    "print(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec Network value Define\n",
    "\"\"\"\n",
    "Negative smapling : \n",
    "Word Embedding 된 Metrics 는 [Voca_size, Embedding _size] 인데 \n",
    "이를 Word2vec Skip-Gram 학습을 위해서 Voca_size 만큼 Softmax를 해야한다\n",
    "당연히 연산량이 크므로 이를 해결하기 위해서 Negative Sampling 사용 \n",
    "쉽게 말해서 모두 연산하지말고 Skip_window size(원래 맞추고자 하는 정답크기) + (정답과 거리가 먼 단어 5~20개를 샘플링하여)\n",
    "Softmax함 이 예제에서는 64개, Negative 샘플링 사이즈는 조절가능\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "embedding_hidden = 128\n",
    "batch_size = 128\n",
    "n_negative_sample= 64\n",
    "text8.skip_window=2\n",
    "n_epochs = 100\n",
    "learning_rate = 0.9\n",
    "text8.reset_index()\n",
    "n_batch = text8.n_batches_wv()\n",
    "\n",
    "input = tf.compat.v1.placeholder(dtype=tf.int32, shape=[batch_size])\n",
    "output = tf.compat.v1.placeholder(dtype=tf.int32, shape=[batch_size, 1])\n",
    "validation = tf.compat.v1.constant(x_valid,dtype=tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding network setup\n",
    "random_uniform_dist = tf.random.uniform(shape=[text8.vocab_len, embedding_hidden], \n",
    "                                       minval=-1.0, maxval=1.0)\n",
    "embedding_dist = tf.compat.v1.Variable(random_uniform_dist, name='embedding_matrix')\n",
    "embedded_table = tf.nn.embedding_lookup(embedding_dist, ids=input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise contrastive estimate loss \n",
    "nce_w = tf.compat.v1.Variable(tf.random.truncated_normal(shape=[text8.vocab_len, embedding_hidden], \n",
    "                                                         stddev=1/tf.sqrt(embedding_hidden*1.0)),\n",
    "                              name='nce_weight')\n",
    "\n",
    "nce_b = tf.compat.v1.Variable(tf.zeros(shape=[text8.vocab_len]), name='nce_biases')\n",
    "\n",
    "loss = tf.compat.v1.reduce_mean(tf.nn.nce_loss(weights=nce_w, biases=nce_b, inputs=embedded_table, labels=output, \n",
    "                     num_sampled=n_negative_sample, num_classes=text8.vocab_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similarity Calculation\n",
    "normal_embedding_table = embedding_dist/tf.sqrt(tf.reduce_sum(tf.square(embedding_dist), axis=1, keepdims=True))\n",
    "validate_embedding_table = tf.nn.embedding_lookup(normal_embedding_table, validation)\n",
    "similarity = tf.matmul(validate_embedding_table, normal_embedding_table, transpose_b=True)\n",
    "\n",
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul_5:0' shape=(8, 253854) dtype=float32>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 ,,,,avg_loss : 135.58712847282112\n",
      "epoch : 1 ,,,,avg_loss : 70.16661318281474\n",
      "epoch : 2 ,,,,avg_loss : 49.144337317706714\n",
      "epoch : 3 ,,,,avg_loss : 36.81560307906483\n",
      "epoch : 4 ,,,,avg_loss : 28.941727477796018\n",
      "epoch : 5 ,,,,avg_loss : 22.32977826975237\n",
      "epoch : 6 ,,,,avg_loss : 19.900982078854266\n",
      "epoch : 7 ,,,,avg_loss : 16.84210065618886\n",
      "epoch : 8 ,,,,avg_loss : 14.306939469463465\n",
      "epoch : 9 ,,,,avg_loss : 12.982754683109828\n",
      "epoch : 10 ,,,,avg_loss : 11.575401850709756\n",
      "epoch : 11 ,,,,avg_loss : 10.639240667937804\n",
      "epoch : 12 ,,,,avg_loss : 9.621933877407994\n",
      "epoch : 13 ,,,,avg_loss : 9.537169499915475\n",
      "epoch : 14 ,,,,avg_loss : 8.99251304809577\n",
      "epoch : 15 ,,,,avg_loss : 8.21186364425954\n",
      "epoch : 16 ,,,,avg_loss : 8.288504419035823\n",
      "epoch : 17 ,,,,avg_loss : 8.135611283406428\n",
      "epoch : 18 ,,,,avg_loss : 7.564091099282437\n",
      "epoch : 19 ,,,,avg_loss : 8.275065974370435\n",
      "epoch : 20 ,,,,avg_loss : 7.696875309709702\n",
      "epoch : 21 ,,,,avg_loss : 7.50614417048638\n",
      "epoch : 22 ,,,,avg_loss : 7.331343930235308\n",
      "epoch : 23 ,,,,avg_loss : 7.154031384610432\n",
      "epoch : 24 ,,,,avg_loss : 6.9653422628697035\n",
      "epoch : 25 ,,,,avg_loss : 7.20000102408174\n",
      "epoch : 26 ,,,,avg_loss : 7.007273180206436\n",
      "epoch : 27 ,,,,avg_loss : 7.117542982913001\n",
      "epoch : 28 ,,,,avg_loss : 6.5625808773711665\n",
      "epoch : 29 ,,,,avg_loss : 6.953761851012075\n",
      "epoch : 30 ,,,,avg_loss : 6.971034056886062\n",
      "epoch : 31 ,,,,avg_loss : 7.101035929543769\n",
      "epoch : 32 ,,,,avg_loss : 7.090278575502137\n",
      "epoch : 33 ,,,,avg_loss : 6.668121777816788\n",
      "epoch : 34 ,,,,avg_loss : 6.53795784516462\n",
      "epoch : 35 ,,,,avg_loss : 7.959602479043779\n",
      "epoch : 36 ,,,,avg_loss : 6.483311620885173\n",
      "epoch : 37 ,,,,avg_loss : 6.173916916683715\n",
      "epoch : 38 ,,,,avg_loss : 6.609533585476503\n",
      "epoch : 39 ,,,,avg_loss : 6.49122171265515\n",
      "epoch : 40 ,,,,avg_loss : 6.626606476078476\n",
      "epoch : 41 ,,,,avg_loss : 6.099432636437486\n",
      "epoch : 42 ,,,,avg_loss : 6.142308247041293\n",
      "epoch : 43 ,,,,avg_loss : 6.1872650511145775\n",
      "epoch : 44 ,,,,avg_loss : 6.070278976520263\n",
      "epoch : 45 ,,,,avg_loss : 6.249008318058962\n",
      "epoch : 46 ,,,,avg_loss : 6.096657909892511\n",
      "epoch : 47 ,,,,avg_loss : 6.135414239979127\n",
      "epoch : 48 ,,,,avg_loss : 6.312505552625031\n",
      "epoch : 49 ,,,,avg_loss : 5.755521781565983\n",
      "epoch : 50 ,,,,avg_loss : 6.1127574150152295\n",
      "epoch : 51 ,,,,avg_loss : 5.976114136849277\n",
      "epoch : 52 ,,,,avg_loss : 5.932222760901929\n",
      "epoch : 53 ,,,,avg_loss : 5.997232522017055\n",
      "epoch : 54 ,,,,avg_loss : 6.615665636757559\n",
      "epoch : 55 ,,,,avg_loss : 6.260682465028594\n",
      "epoch : 56 ,,,,avg_loss : 5.962285382784922\n",
      "epoch : 57 ,,,,avg_loss : 6.476375367821195\n",
      "epoch : 58 ,,,,avg_loss : 6.431894917591497\n",
      "epoch : 59 ,,,,avg_loss : 6.015495341036336\n",
      "epoch : 60 ,,,,avg_loss : 5.9271154772796475\n",
      "epoch : 61 ,,,,avg_loss : 5.815318853790208\n",
      "epoch : 62 ,,,,avg_loss : 5.7854525146313645\n",
      "epoch : 63 ,,,,avg_loss : 5.866118691422034\n",
      "epoch : 64 ,,,,avg_loss : 5.795609461016807\n",
      "epoch : 65 ,,,,avg_loss : 6.119932371441067\n",
      "epoch : 66 ,,,,avg_loss : 5.777073372660775\n",
      "epoch : 67 ,,,,avg_loss : 4.822674552539473\n",
      "epoch : 68 ,,,,avg_loss : 4.746250412287402\n",
      "epoch : 69 ,,,,avg_loss : 4.724279245822886\n",
      "epoch : 70 ,,,,avg_loss : 4.751577738433911\n",
      "epoch : 71 ,,,,avg_loss : 4.701243557114824\n",
      "epoch : 72 ,,,,avg_loss : 4.68117556250943\n",
      "epoch : 73 ,,,,avg_loss : 4.749522093745475\n",
      "epoch : 74 ,,,,avg_loss : 4.74905238542903\n",
      "epoch : 75 ,,,,avg_loss : 4.710818325036473\n",
      "epoch : 76 ,,,,avg_loss : 4.638700414413283\n",
      "epoch : 77 ,,,,avg_loss : 4.674442726391707\n",
      "epoch : 78 ,,,,avg_loss : 4.701588620997653\n",
      "epoch : 79 ,,,,avg_loss : 4.648400272916677\n",
      "epoch : 80 ,,,,avg_loss : 4.704763723634074\n",
      "epoch : 81 ,,,,avg_loss : 4.729325785549133\n",
      "epoch : 82 ,,,,avg_loss : 4.652693313079173\n",
      "epoch : 83 ,,,,avg_loss : 4.581661372715574\n",
      "epoch : 84 ,,,,avg_loss : 4.70242893782999\n",
      "epoch : 85 ,,,,avg_loss : 4.648750846847644\n",
      "epoch : 86 ,,,,avg_loss : 4.662640169165198\n",
      "epoch : 87 ,,,,avg_loss : 4.694780594432108\n",
      "epoch : 88 ,,,,avg_loss : 4.723872636086521\n",
      "epoch : 89 ,,,,avg_loss : 4.708395544696924\n",
      "epoch : 90 ,,,,avg_loss : 4.695158225896078\n",
      "epoch : 91 ,,,,avg_loss : 4.6518720808921294\n",
      "epoch : 92 ,,,,avg_loss : 4.668440139389856\n",
      "epoch : 93 ,,,,avg_loss : 4.6956488215676835\n",
      "epoch : 94 ,,,,avg_loss : 4.71851344314177\n",
      "epoch : 95 ,,,,avg_loss : 4.629498522719288\n",
      "epoch : 96 ,,,,avg_loss : 4.655000007188627\n",
      "epoch : 97 ,,,,avg_loss : 4.7105313340221695\n",
      "epoch : 98 ,,,,avg_loss : 4.703419191949727\n",
      "epoch : 99 ,,,,avg_loss : 4.687003047792105\n"
     ]
    }
   ],
   "source": [
    "from datasetslib import nputil\n",
    "text8.reset_index()\n",
    "\n",
    "with tf.compat.v1.Session() as sess: \n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    for i in range(n_epochs):\n",
    "        avg_loss = 0\n",
    "        for batch in range(n_batch):\n",
    "            x_train, label = text8.next_batch_sg()\n",
    "            label = nputil.to2d(label, unit_axis=1)\n",
    "            res_loss, _ = sess.run([loss, optimizer], feed_dict={input:x_train, output:label})\n",
    "            avg_loss+=res_loss\n",
    "        print(f'epoch : {i} ,,,,avg_loss : {avg_loss/n_batch}')\n",
    "    \n",
    "    similarity_score = sess.run(similarity)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'ar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-51d24948a451>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#similarity score calculation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msimilarity_score\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msimilarity_cosine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimilarity_score\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'ar'"
     ]
    }
   ],
   "source": [
    "#similarity score calculation\n",
    "top_k = 5\n",
    "similarity_score.ar\n",
    "for i in range(valid_size):\n",
    "    similarity_cosine = similarity_score[i]\n",
    "    top_rank_5 = similarity_cosine.argsort(similarity_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5\n",
    "\n",
    "for i in range(valid_size):\n",
    "    similarity_cosine = similarity_score[i]\n",
    "    top_rank_5 = np.argsort(similarity_cosine)\n",
    "    start_num = text8.vocab_len - top_k-1\n",
    "    end_num = text8.vocab_len-1\n",
    "    top_rank_5_index = top_rank_5.tolist()[start_num:end_num]\n",
    "\n",
    "    similar_str = 'Similar to {0:}:'.format(text8.id2word[x_valid[i]])\n",
    "    for rank_word in top_rank_5_index:\n",
    "        similar_str = '{0:} {1:},'.format(similar_str, text8.id2word[rank_word])\n",
    "    \n",
    "    print(similar_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
