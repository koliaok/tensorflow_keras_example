{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Embedding \"Word2Vec\"\n",
    "==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#System path 등록\n",
    "import sys  \n",
    "sys.path.insert(0, '../../../tensorflow_keras_example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimhyungrak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kimhyungrak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kimhyungrak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kimhyungrak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kimhyungrak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kimhyungrak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists: ./datasets/text8/text8.zip\n",
      "Train: [5233 3083   11    5  194]\n",
      "Vocabulary Length =  253854\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from datasetslib.text8 import Text8\n",
    "text8 = Text8()\n",
    "# downloads data, converts words to ids, converts files to a list of ids\n",
    "text8.load_data()\n",
    "print('Train:', text8.part['train'][0:5])\n",
    "# print(text8.part['test'][0:5])\n",
    "# print(text8.part['valid'][0:5])\n",
    "print('Vocabulary Length = ', text8.vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The skip-gram pairs : target,context\n",
      "( 11 as , 5233 anarchism )\n",
      "( 11 as , 3083 originated )\n",
      "( 11 as , 5 a )\n",
      "( 11 as , 194 term )\n",
      "( 5 a , 3083 originated )\n",
      "( 5 a , 11 as )\n",
      "( 5 a , 194 term )\n",
      "( 5 a , 1 of )\n",
      "( 194 term , 11 as )\n",
      "( 194 term , 5 a )\n"
     ]
    }
   ],
   "source": [
    "#skip_gram example\n",
    "text8.skip_window = 2\n",
    "text8.reset_index()\n",
    "# in skip-gram input is the target word and output is the context word\n",
    "x_batch, y_batch = text8.next_batch_sg()\n",
    "\n",
    "print('The skip-gram pairs : target,context')\n",
    "for i in range(5 * text8.skip_window):\n",
    "    print('(', x_batch[i], text8.id2word[x_batch[i]],\n",
    "          ',', y_batch[i], text8.id2word[y_batch[i]], ')')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55 46 71 30 70 64 62  1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "valid_size = 8\n",
    "x_valid = np.random.choice(valid_size * 10, valid_size, replace=False)\n",
    "print(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec Network value Define\n",
    "\"\"\"\n",
    "Negative smapling : \n",
    "Word Embedding 된 Metrics 는 [Voca_size, Embedding _size] 인데 \n",
    "이를 Word2vec Skip-Gram 학습을 위해서 Voca_size 만큼 Softmax를 해야한다\n",
    "당연히 연산량이 크므로 이를 해결하기 위해서 Negative Sampling 사용 \n",
    "쉽게 말해서 모두 연산하지말고 Skip_window size(원래 맞추고자 하는 정답크기) + (정답과 거리가 먼 단어 5~20개를 샘플링하여)\n",
    "Softmax함 이 예제에서는 64개, Negative 샘플링 사이즈는 조절가능\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "embedding_hidden = 128\n",
    "batch_size = 128\n",
    "n_negative_sample= 64\n",
    "text8.skip_window=2\n",
    "n_epochs = 100\n",
    "learning_rate = 0.9\n",
    "text8.reset_index()\n",
    "n_batch = text8.n_batches_wv()\n",
    "\n",
    "input = tf.compat.v1.placeholder(dtype=tf.int32, shape=[batch_size])\n",
    "output = tf.compat.v1.placeholder(dtype=tf.int32, shape=[batch_size, 1])\n",
    "validation = tf.compat.v1.constant(x_valid,dtype=tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/kimhyungrak/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#embedding network setup\n",
    "random_uniform_dist = tf.random.uniform(shape=[text8.vocab_len, embedding_hidden], \n",
    "                                       minval=-1.0, maxval=1.0)\n",
    "embedding_dist = tf.compat.v1.Variable(random_uniform_dist, name='embedding_matrix')\n",
    "embedded_table = tf.nn.embedding_lookup(embedding_dist, ids=input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise contrastive estimate loss \n",
    "nce_w = tf.compat.v1.Variable(tf.random.truncated_normal(shape=[text8.vocab_len, embedding_hidden], \n",
    "                                                         stddev=1/tf.sqrt(embedding_hidden*1.0)),\n",
    "                              name='nce_weight')\n",
    "\n",
    "nce_b = tf.compat.v1.Variable(tf.zeros(shape=[text8.vocab_len]), name='nce_biases')\n",
    "\n",
    "loss = tf.compat.v1.reduce_mean(tf.nn.nce_loss(weights=nce_w, biases=nce_b, inputs=embedded_table, labels=output, \n",
    "                     num_sampled=n_negative_sample, num_classes=text8.vocab_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/kimhyungrak/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "#Similarity Calculation\n",
    "normal_embedding_table = embedding_dist/tf.sqrt(tf.reduce_sum(tf.square(embedding_dist), axis=1, keepdims=True))\n",
    "validate_embedding_table = tf.nn.embedding_lookup(normal_embedding_table, validation)\n",
    "similarity = tf.matmul(validate_embedding_table, normal_embedding_table, transpose_b=True)\n",
    "\n",
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul_1:0' shape=(8, 253854) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 ,,,,avg_loss : 135.6713991018479\n",
      "epoch : 1 ,,,,avg_loss : 70.3975912521899\n",
      "epoch : 2 ,,,,avg_loss : 49.271941610796546\n",
      "epoch : 3 ,,,,avg_loss : 36.78707033648253\n",
      "epoch : 4 ,,,,avg_loss : 28.28141912717261\n",
      "epoch : 5 ,,,,avg_loss : 22.33857558339278\n",
      "epoch : 6 ,,,,avg_loss : 20.012945121968567\n",
      "epoch : 7 ,,,,avg_loss : 17.053423297838076\n",
      "epoch : 8 ,,,,avg_loss : 14.236869316268196\n",
      "epoch : 9 ,,,,avg_loss : 13.010429044339132\n",
      "epoch : 10 ,,,,avg_loss : 11.527509298759822\n",
      "epoch : 11 ,,,,avg_loss : 10.563612657307496\n",
      "epoch : 12 ,,,,avg_loss : 9.639623155241594\n",
      "epoch : 13 ,,,,avg_loss : 9.523887363576673\n",
      "epoch : 14 ,,,,avg_loss : 8.991761600857002\n",
      "epoch : 15 ,,,,avg_loss : 8.195179055224628\n",
      "epoch : 16 ,,,,avg_loss : 8.286386729759576\n",
      "epoch : 17 ,,,,avg_loss : 8.157288571108609\n",
      "epoch : 18 ,,,,avg_loss : 7.5496608483568926\n",
      "epoch : 19 ,,,,avg_loss : 8.370453141145013\n",
      "epoch : 20 ,,,,avg_loss : 7.706771182731373\n",
      "epoch : 21 ,,,,avg_loss : 7.589919824703618\n",
      "epoch : 22 ,,,,avg_loss : 7.382411453529133\n",
      "epoch : 23 ,,,,avg_loss : 7.17613183240727\n",
      "epoch : 24 ,,,,avg_loss : 6.978583053242842\n",
      "epoch : 25 ,,,,avg_loss : 7.2023018028209504\n",
      "epoch : 26 ,,,,avg_loss : 7.012134682813559\n",
      "epoch : 27 ,,,,avg_loss : 7.14609258612538\n",
      "epoch : 28 ,,,,avg_loss : 6.567247778718302\n",
      "epoch : 29 ,,,,avg_loss : 6.921021228553788\n",
      "epoch : 30 ,,,,avg_loss : 6.976531063058553\n",
      "epoch : 31 ,,,,avg_loss : 7.100894295171361\n",
      "epoch : 32 ,,,,avg_loss : 7.133468182893696\n",
      "epoch : 33 ,,,,avg_loss : 6.6438227163470875\n",
      "epoch : 34 ,,,,avg_loss : 6.5685921181077855\n",
      "epoch : 35 ,,,,avg_loss : 7.965166221738643\n",
      "epoch : 36 ,,,,avg_loss : 6.46879202948035\n",
      "epoch : 37 ,,,,avg_loss : 6.155659005579417\n",
      "epoch : 38 ,,,,avg_loss : 6.485310929813772\n",
      "epoch : 39 ,,,,avg_loss : 6.503384093593842\n",
      "epoch : 40 ,,,,avg_loss : 6.5802105652582625\n",
      "epoch : 41 ,,,,avg_loss : 6.070262075672833\n",
      "epoch : 42 ,,,,avg_loss : 6.162661937740315\n",
      "epoch : 43 ,,,,avg_loss : 6.2152114226189275\n",
      "epoch : 44 ,,,,avg_loss : 6.062807196808534\n",
      "epoch : 45 ,,,,avg_loss : 6.281322814287469\n",
      "epoch : 46 ,,,,avg_loss : 6.093953559330119\n",
      "epoch : 47 ,,,,avg_loss : 6.12834844556771\n",
      "epoch : 48 ,,,,avg_loss : 6.315199402174563\n",
      "epoch : 49 ,,,,avg_loss : 5.773374841285966\n",
      "epoch : 50 ,,,,avg_loss : 6.085943697500397\n",
      "epoch : 51 ,,,,avg_loss : 5.98188112938398\n",
      "epoch : 52 ,,,,avg_loss : 5.909778143321993\n",
      "epoch : 53 ,,,,avg_loss : 6.00217268407435\n",
      "epoch : 54 ,,,,avg_loss : 6.619672742204959\n",
      "epoch : 55 ,,,,avg_loss : 6.2571632193304705\n",
      "epoch : 56 ,,,,avg_loss : 5.964284544001909\n",
      "epoch : 57 ,,,,avg_loss : 6.453138141261166\n",
      "epoch : 58 ,,,,avg_loss : 6.431056336898486\n",
      "epoch : 59 ,,,,avg_loss : 6.02358244782557\n",
      "epoch : 60 ,,,,avg_loss : 5.93721687390957\n",
      "epoch : 61 ,,,,avg_loss : 5.824757911990041\n",
      "epoch : 62 ,,,,avg_loss : 5.8008500963103815\n",
      "epoch : 63 ,,,,avg_loss : 5.873009366725829\n",
      "epoch : 64 ,,,,avg_loss : 5.838827505474793\n",
      "epoch : 65 ,,,,avg_loss : 6.103526258011749\n",
      "epoch : 66 ,,,,avg_loss : 5.753040843639961\n",
      "epoch : 67 ,,,,avg_loss : 4.849106007582963\n",
      "epoch : 68 ,,,,avg_loss : 4.736223989433552\n",
      "epoch : 69 ,,,,avg_loss : 4.731260186124197\n",
      "epoch : 70 ,,,,avg_loss : 4.745799344414362\n",
      "epoch : 71 ,,,,avg_loss : 4.69798671262197\n",
      "epoch : 72 ,,,,avg_loss : 4.6874075347705135\n",
      "epoch : 73 ,,,,avg_loss : 4.750925924702239\n",
      "epoch : 74 ,,,,avg_loss : 4.741609046571013\n",
      "epoch : 75 ,,,,avg_loss : 4.711445759777102\n",
      "epoch : 76 ,,,,avg_loss : 4.64122080328004\n",
      "epoch : 77 ,,,,avg_loss : 4.68217764794075\n",
      "epoch : 78 ,,,,avg_loss : 4.697239865092875\n",
      "epoch : 79 ,,,,avg_loss : 4.655450326592046\n",
      "epoch : 80 ,,,,avg_loss : 4.707846053068649\n",
      "epoch : 81 ,,,,avg_loss : 4.721115212998364\n",
      "epoch : 82 ,,,,avg_loss : 4.645707786458944\n",
      "epoch : 83 ,,,,avg_loss : 4.58059184532002\n",
      "epoch : 84 ,,,,avg_loss : 4.6941177871752675\n",
      "epoch : 85 ,,,,avg_loss : 4.645419407059193\n",
      "epoch : 86 ,,,,avg_loss : 4.676316209742353\n",
      "epoch : 87 ,,,,avg_loss : 4.68439521509412\n",
      "epoch : 88 ,,,,avg_loss : 4.736695762075563\n",
      "epoch : 89 ,,,,avg_loss : 4.7206531889019026\n",
      "epoch : 90 ,,,,avg_loss : 4.690702419237723\n",
      "epoch : 91 ,,,,avg_loss : 4.653151423400301\n",
      "epoch : 92 ,,,,avg_loss : 4.659541091381511\n",
      "epoch : 93 ,,,,avg_loss : 4.704372725760883\n",
      "epoch : 94 ,,,,avg_loss : 4.707987068670901\n",
      "epoch : 95 ,,,,avg_loss : 4.633725608396097\n",
      "epoch : 96 ,,,,avg_loss : 4.663870804609221\n",
      "epoch : 97 ,,,,avg_loss : 4.700293313762605\n",
      "epoch : 98 ,,,,avg_loss : 4.70440261882781\n",
      "epoch : 99 ,,,,avg_loss : 4.675812703142609\n"
     ]
    }
   ],
   "source": [
    "from datasetslib import nputil\n",
    "text8.reset_index()\n",
    "\n",
    "with tf.compat.v1.Session() as sess: \n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    for i in range(n_epochs):\n",
    "        avg_loss = 0\n",
    "        for batch in range(n_batch):\n",
    "            x_train, label = text8.next_batch_sg()\n",
    "            label = nputil.to2d(label, unit_axis=1)\n",
    "            res_loss, _ = sess.run([loss, optimizer], feed_dict={input:x_train, output:label})\n",
    "            avg_loss+=res_loss\n",
    "        print(f'epoch : {i} ,,,,avg_loss : {avg_loss/n_batch}')\n",
    "    \n",
    "    similarity_score = sess.run(similarity)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar to many: numerous, all, various, several, some,\n",
      "Similar to they: these, she, you, we, he,\n",
      "Similar to i: iii, g, we, you, ii,\n",
      "Similar to an: the, any, a, recitative, another,\n",
      "Similar to world: mastitis, city, vernius, europe, country,\n",
      "Similar to time: cebus, point, albury, cichlid, period,\n",
      "Similar to into: across, goldfinches, from, within, through,\n",
      "Similar to of: liceo, busan, cebus, landsmannschaft, including,\n"
     ]
    }
   ],
   "source": [
    "#similarity score calculation Test\n",
    "top_k = 5\n",
    "\n",
    "for i in range(valid_size):\n",
    "    similarity_cosine = similarity_score[i]\n",
    "    top_rank_5 = np.argsort(similarity_cosine)\n",
    "    start_num = text8.vocab_len - top_k-1\n",
    "    end_num = text8.vocab_len-1\n",
    "    top_rank_5_index = top_rank_5.tolist()[start_num:end_num]\n",
    "\n",
    "    similar_str = 'Similar to {0:}:'.format(text8.id2word[x_valid[i]])\n",
    "    for rank_word in top_rank_5_index:\n",
    "        similar_str = '{0:} {1:},'.format(similar_str, text8.id2word[rank_word])\n",
    "    \n",
    "    print(similar_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
